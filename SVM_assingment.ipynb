{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZcg7M0-mEpc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & NAVIE BAYES"
      ],
      "metadata": {
        "id": "S_kITSTLmOU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "1. ans .\n",
        "Information Gain (IG) measures the reduction in entropy (uncertainty or impurity) within a dataset after splitting it based on a specific feature. It is used in Decision Tree algorithms (like ID3) to select the best feature for splitting nodes, maximizing homogeneity in resulting child nodes. The feature with the highest information gain is chosen for the split to optimize classification accuracy.\n",
        "* Decision Tree Application:\n",
        "* Root Node Selection: The algorithm calculates the Information Gain for every feature and selects the one that provides the highest gain."
      ],
      "metadata": {
        "id": "uoyvsI0cmX0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2  * Gini Impurity and Entropy are both metrics used to measure node impurity in decision trees, guiding splits by reducing disorder. Gini (0 to 0.5) calculates the probability of incorrect classification, while Entropy (0 to 1) measures information disorder. Gini is generally faster, whereas Entropy can produce slightly more balanced trees.\n",
        "\n",
        "*Use Gini Impurity when computational efficiency is critical or when dealing with imbalanced datasets.\n",
        "\n",
        "*. Use Entropy if the tree requires a more granular, theoretically justified measure of information gain.\n"
      ],
      "metadata": {
        "id": "hwxCaAm_nwWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ans 3.\n",
        "\n",
        " Pre-pruning, or early stopping, is a decision tree optimization technique that halts tree growth during training before it becomes too complex, preventing overfitting. It works by setting restrictions—such as maximum depth or minimum samples per split—to stop building branches, resulting in smaller, more interpretable, and faster-trained models.\n",
        "\n",
        "* Key Aspects of Pre-Pruning:\n",
        "\n",
        "* How it Works: Instead of allowing the tree to grow until every node is pure, pre-pruning checks constraints at each node before splitting. If a split fails to meet criteria (e.g., node samples are too few), it becomes a leaf node.\n",
        "\n",
        "* Advantages: Significantly reduces training time, memory usage, and the risk of overfitting.\n",
        "\n",
        "* Disadvantages: It can lead to underfitting if the stopping criteria are too strict, preventing the model from capturing necessary patterns."
      ],
      "metadata": {
        "id": "jXMLsvKGpB3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Ans 4. To train a Decision Tree Classifier using the Gini impurity criterion and print the feature importances, you can use the popular Python library scikit-learn [1].\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "KchKATSKpwXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_gini_decision_tree():\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree Classifier using Gini impurity and prints feature importances.\n",
        "    \"\"\"\n",
        "    # 1. Load a sample dataset (Iris dataset is built into sklearn)\n",
        "    print(\"Loading the Iris dataset...\")\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    feature_names = iris.feature_names\n",
        "    print(f\"Features: {feature_names}\\n\")\n",
        "\n",
        "    # 2. Split the data into training and testing sets (optional but good practice)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    # 3. Instantiate the Decision Tree Classifier with Gini criterion (default, but explicit)\n",
        "    # criterion='gini' specifies the use of Gini impurity\n",
        "    print(\"Initializing Decision Tree Classifier with criterion='gini'...\")\n",
        "    dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "    # 4. Train the classifier\n",
        "    print(\"Training the classifier...\")\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "    print(\"Training complete.\\n\")\n",
        "\n",
        "    # 5. Print the feature importances\n",
        "    print(\"Feature Importances:\")\n",
        "    importances = dt_classifier.feature_importances_\n",
        "\n",
        "    # Pair feature names with their importances for better readability\n",
        "    feature_importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "    feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    # Format importance values to a specific number of decimal places\n",
        "    feature_importances_df['Importance'] = feature_importances_df['Importance'].round(4)\n",
        "\n",
        "    print(feature_importances_df.to_string(index=False))\n",
        "\n",
        "    # Optional: Evaluate the model's accuracy\n",
        "    accuracy = dt_classifier.score(X_test, y_test)\n",
        "    print(f\"\\nModel Accuracy on test set: {accuracy:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_gini_decision_tree()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5xrNXOPqGBM",
        "outputId": "f3dfe736-aca9-4c4e-d194-1ffce73883fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the Iris dataset...\n",
            "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "\n",
            "Initializing Decision Tree Classifier with criterion='gini'...\n",
            "Training the classifier...\n",
            "Training complete.\n",
            "\n",
            "Feature Importances:\n",
            "          Feature  Importance\n",
            "petal length (cm)      0.8933\n",
            " petal width (cm)      0.0876\n",
            " sepal width (cm)      0.0191\n",
            "sepal length (cm)      0.0000\n",
            "\n",
            "Model Accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.\n",
        " Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression, finding the optimal boundary (hyperplane) to separate different classes in data by maximizing the margin between them, working well with complex, high-dimensional data by mapping it to higher spaces using kernel functions. It's used in text classification (spam detection), image recognition, and bioinformatic.\n",
        "\n",
        " *How SVM Works (Classification).\n",
        "\n",
        " * Find the Hyperplane: SVM identifies the best line (or plane/hyperplane in higher dimensions) that separates data points of different classes.\n",
        "\n",
        " * Maximize the Margin: It seeks the hyperplane with the widest possible gap (margin) to the nearest data points from each class, ensuring better generalization to new data."
      ],
      "metadata": {
        "id": "SflW8ZMzqMDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6  *  . The Kernel Trick in SVM is a powerful technique that allows Support Vector Machines to efficiently classify non-linearly separable data by implicitly mapping it into a higher-dimensional space, where it becomes linearly separable, without actually computing the coordinates in that new space, saving massive computational effort. It replaces complex, high-dimensional dot products with a simple kernel function (like the Radial Basis Function or Polynomial kernel) that operates in the original, lower-dimensional space, providing a curved boundary in the input space.\n",
        "\n",
        "* How it works:\n",
        "The Problem: Data that isn't linearly separable in its original form (e.g., a circle of points inside another) requires complex, curved boundaries, which linear classifiers struggle with.\n",
        "\n",
        "* The Idea (Mapping): The trick maps data points (e.g., from 2D to 3D or higher) into a new feature space where they can be separated by a straight line (hyperplane)."
      ],
      "metadata": {
        "id": "ycREFrhUrou6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS 7 *"
      ],
      "metadata": {
        "id": "KCOsjGXbsplw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "# Using a common split of 80% train, 20% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 3. Train an SVM classifier with a Linear kernel\n",
        "print(\"Training Linear SVM...\")\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "print(\"Linear SVM training complete.\")\n",
        "\n",
        "# 4. Train an SVM classifier with an RBF kernel\n",
        "print(\"\\nTraining RBF SVM...\")\n",
        "# RBF kernel often benefits from scaling, but for this basic example we skip it\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "print(\"RBF SVM training complete.\")\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# 6. Compare accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(\"\\n--- Accuracy Comparison ---\")\n",
        "print(f\"Linear Kernel SVM Accuracy: {accuracy_linear:.4f}\")\n",
        "print(f\"RBF Kernel SVM Accuracy: {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Optional: Determine which performed better\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"\\nThe Linear Kernel SVM performed better on this dataset split.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"\\nThe RBF Kernel SVM performed better on this dataset split.\")\n",
        "else:\n",
        "    print(\"\\nBoth SVMs performed identically on this dataset split.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2qqmeIxCtXlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS 8*\n",
        "\n",
        "The Naive Bayes classifier is a fast, simple probabilistic model that predicts class membership by applying Bayes' theorem with a strong, unrealistic assumption that all features are conditionally independent of each other, given the class. It's called \"naive\" because this assumption of feature independence (e.g., words in an email don't affect each other) rarely holds true in real data, yet the model performs surprisingly well in practice, especially for text classification like spam filtering\n",
        "\n",
        "* How it works.\n",
        "\n",
        "* Based on Bayes' Theorem: It calculates the probability of a class given certain features\n",
        "\n",
        "* The \"Naive\" Assumption: It simplifies calculations by assuming each feature (like a word) contributes independently to the probability, ignoring complex relationships between features\n",
        "* Efficient & Simple: This independence assumption drastically reduces computational complexity, making it very fast and easy to implement, even for high-dimensional data."
      ],
      "metadata": {
        "id": "AEzO1IRgtajR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS9*\n",
        "\n",
        " Gaussian, Multinomial, and Bernoulli Naïve Bayes differ primarily in the type of data they handle: Gaussian assumes continuous, normally distributed data (e.g., height); Multinomial models discrete counts, ideal for text classification; and Bernoulli handles binary (0/1) features, focusing on feature presence or absence.\n",
        "\n",
        " * Data Type: Continuous/Numerical (real-valued).\n",
        " * Assumption: Features follow a normal (Gaussian) distribution.\n",
        " *Characteristic: Calculates mean and standard deviation for features to calculate probabilities.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "yqUW5g4kt-SZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS 10 *\n",
        "\n",
        " This Python program uses scikit-learn to load the breast cancer dataset, splits it into training and testing sets, trains a GaussianNB classifier, and evaluates its accuracy. The model predicts whether tumors are malignant or benign based on 30 features, often achieving over 90% accuracy.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "b86vUCmVue8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize the Gaussian Naive Bayes classifier\n",
        "clf = GaussianNB()\n",
        "\n",
        "# 4. Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 6. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9a8VXZYu4m0",
        "outputId": "1d407c77-f0e3-44bc-9017-3515ba7d0bf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9737\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n",
            "Confusion Matrix:\n",
            "[[40  3]\n",
            " [ 0 71]]\n"
          ]
        }
      ]
    }
  ]
}